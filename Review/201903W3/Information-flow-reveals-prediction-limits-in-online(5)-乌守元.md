该系列是阅读/翻译 nature 上的论文[《Information flow reveals prediction limits in online social activity》](https://doi.org/10.1038/s41562-018-0510-5)的记录，每周将不定量翻译，也欢迎大家提出建议。

The ability of a machine learning method to accurately profile individuals from their online traces is reflected in the predictability of their written text. Indeed, with a language model trained to predict the words a user will post online, in principle, one can construct a profile of the user by evaluating the likelihoods of various words to be posted, such as terms related to politics. Thus, quantifying the predictive information contained within a user's text allows us to understand the potential accuracy such methods can potentially achieve given a user's data.

个人的书面文本的可预测性反映了机器学习方法准确地讲个人从他们的线上痕迹描述出来。事实上，通过悬链以预测用户将要在线发布的单词的语言模型，原则上，可以通过评估要发布的各种单词的可能性来构建用户的简档，例如与政治相关的术语。因此，量化用户文本中包含的预测信息使我们能够理解这些方法在给定用户数据的情况下可能实现的潜在准确性。



A text's predictive information can be characterized by three related quantities, the entropy rate h, the perplexity 2^h^, and the predictability Π . The entropy rate quantifies the average uncertainty one has about future words given the text one has already observed(Fig. 1a). Higher entropies correspond to less predictable text and reflect individuals whose interests are more difficult to predict. In the context of language models, it is also common to consider the perplexity. Whereas the entropy rate specifies how many bits h are needed on average to express subsequent unseen words given the preceding text, the perplexity tells us that our remaining uncertainty about those unseen words is equivalent to that of choosing uniformly at random from among 2^h^ possibilities. For example, if h = 6 bits (typical of individuals in our data set), the perplexity is 64 words, which is a significant reduction from choosing randomly over the entire vocabulary (social media users have ~5,000-word vocabularies on average; see Supplementary Note 1.3 for full distributions). Finally, the predictability Π , given via Fano's inequality^21^ , is the probability that an ideal predictive algorithm will correctly predict the subsequent word given the preceding text. Repeated, accurate predictions of future words indicate that the available information can be used to build profiles and predictive models of a user's writing (see below for subsequent discussion), and estimating Π allows us to fundamentally bound the usefulness of the information present in a user’s writing without depending on the results of specific predictive algorithms. We emphasize that the information-theoretic predictability as defined here is distinct from prediction, in that it does not actually make predictions about future text. Instead, this predictability provides a method-independent upper bound on prediction accuracy.

文本的预测信息可以用三个相关量来表征，即熵率 h，困惑度 2^h^ 和可预测性 Π。熵率量化了人们对已经观察过的文本的未来单词的平均不确定性（图 1a）较高的熵对应于较不可预测的文本，并反映其兴趣更难以预测的个体。 在语言模型的背景下，考虑困惑度也是很常见的。而熵率指定平均需要多少 h bit 来表示给定前面文本的后续看不见的单词，困惑度告诉我们，我们对那些看不见的单词的不确定性等同于从 2^h^ 可能性中随机均匀选择。例如，如果 h = 6 bit （我们的数据集中典型的个体），则困惑度是 64 个单词，这是在整个词汇表中随机选择是一个显著的减少（社交媒体用户平均拥有约 5,000 字的词汇; 有关补充信息，请参阅补充说明1.3）。最后，通过 Fano 不等式给出的可预测性 Π 是理想预测算法在给定前一文本的情况下正确预测后续单词的概率。再次说明，对未来单词的准确预测表明可获得的信息可用于构建用户书写的简档和预测模型（请参阅下面的后续讨论），并且估计 Π 允许我们从根本上约束用户书写中存在的信息的有用性，而不依赖于特定预测算法的结果。我们强调，这里定义的信息理论可预测性与预测不同，因为它实际上并未对未来文本做出预测。 相反，这种可预测性提供了与预测准确性无关的方法上限。

